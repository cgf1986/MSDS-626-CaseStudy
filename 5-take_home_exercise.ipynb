{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "801a0399-8c66-41ff-bd38-ef8a9d2b8c17",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be36a70e-b45a-443c-b3ae-8a951077ca00",
   "metadata": {},
   "source": [
    "### We conducted an experiment to improve the acceptance rate of our product. The results of the experiment are in outcomes.csv. \n",
    "### Did the treatment improve the acceptance rate? Should we launch it to all of our customers?\n",
    "* Separate the treatment and control groups according to 'treated' values\n",
    "* Find group sizes and number of successes in each group\n",
    "* Perform a 2 Sample Proportions z-test to determine significant statistical differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411859d2-32f9-4121-a907-69a6c063ba36",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "considerable-infection",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T18:53:27.570579Z",
     "start_time": "2022-01-27T18:53:27.527377Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'outcomes.csv': 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "df = pd.read_csv('outcomes.csv')\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in 'outcomes.csv': {}\\n\".format(df.isna().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d347d41-6b7a-4afd-b54d-f16ebe6ea51f",
   "metadata": {},
   "source": [
    "## Separate the treatment and control groups according to 'treated' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d1a8a50-a118-4a71-9ec5-474abb4310ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treatment group:\n",
      "size       = 25158\n",
      "successes  = 4334\n",
      "fails      = 20824\n",
      "proportion = 0.172\n",
      "\n",
      "Control group:\n",
      "size       = 24842\n",
      "successes  = 4156\n",
      "fails      = 20686\n",
      "proportion = 0.167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Treatment Group\n",
    "df_treatment = df[df.treated]\n",
    "df_treatment.reset_index(inplace=True, drop=True)\n",
    "\n",
    "size_treatment = df_treatment.shape[0]\n",
    "success_treatment = df_treatment.accepted.value_counts()[True]\n",
    "print(\"Treatment group:\")\n",
    "print(\"size       = {}\".format(size_treatment))\n",
    "print(\"successes  = {}\".format(success_treatment))\n",
    "print(\"fails      = {}\".format(df_treatment.accepted.value_counts()[False]))\n",
    "print(\"proportion = {:.3f}\\n\".format(success_treatment/size_treatment))\n",
    "\n",
    "# Control Group\n",
    "df_control = df[~df.treated]\n",
    "df_control.reset_index(inplace=True, drop=True)\n",
    "\n",
    "size_control = df_control.shape[0]\n",
    "success_control = df_control.accepted.value_counts()[True]\n",
    "print(\"Control group:\")\n",
    "print(\"size       = {}\".format(size_control))\n",
    "print(\"successes  = {}\".format(success_control))\n",
    "print(\"fails      = {}\".format(df_control.accepted.value_counts()[False]))\n",
    "print(\"proportion = {:.3f}\\n\".format(success_control/size_control))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073cf671-7554-4065-8668-7ae3fc2dc1e7",
   "metadata": {},
   "source": [
    "## Proportions z-test to determine significant statistical differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cb3fdc0-29ba-4ed6-907a-ba15e3086f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zstat = 1.481, p-value = 0.139\n",
      "\n",
      " We failed to reject the null hypothesis, then the two proportions are the same --> P1 = P2\n",
      " No significant statistical difference between treatment and control groups\n"
     ]
    }
   ],
   "source": [
    "# Proportions z-test : Significant statistical difference between the 2 populations\n",
    "# H0: Null Hypothesis: Two proportions are the same P1 = P2\n",
    "# H1: Alternative Hypothesis: Two proportions are not the same \n",
    "\n",
    "significance = 0.05  # alpha\n",
    "successes = np.array([ success_treatment, success_control ])\n",
    "trials    = np.array([ size_treatment, size_control ])\n",
    "\n",
    "zstat, pvalue = proportions_ztest(count=successes, nobs=trials, alternative=\"two-sided\")\n",
    "print(\"zstat = {:.3f}, p-value = {:.3f}\\n\".format(zstat, pvalue))\n",
    "\n",
    "if pvalue > significance:\n",
    "    print(\" We failed to reject the null hypothesis, then the two proportions are the same --> P1 = P2\")\n",
    "    print(\" No significant statistical difference between treatment and control groups\")\n",
    "else:\n",
    "    print(\"There is a significant statistical difference between treatment and control groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b4e332-844b-4968-a646-fc126f63daf6",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Conclusion: Even if the acceptance rate is slightly better in the treatment group (0.005),  the p-value obtained from the statistical test is greater than the chosen threshold (0.05), which means that there isn't a significant statistical difference between the groups. </font> \n",
    "\n",
    "### <font color=\"blue\">Conclusion: I wouldn't launch it to all the customers yet. Instead, I would estimate the cost of performing the experiment on more businesses which would increase the robustness of the statistical test. I would also explore possible error sources </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4eb9e7-0e42-4a81-8780-c1634cb4a06f",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803c7b5e-2b2c-4357-b2d3-226bd732294e",
   "metadata": {},
   "source": [
    "### Suppose we now obtain some data about the pre-experiment characteristics of the businesses in the experiment. It is provided in pre_experiment.csv. \n",
    "### What do you conclude about the experiment based on this data? Were the test/control groups randomly selected?\n",
    "\n",
    "* Using the data in pre_experiment.csv, predictions of whether or not a business was included in the experiment ('treated' variable in outcomes.csv) will be made using Logistic Regression with L2 and L1 regularization. \n",
    "* If the groups were in fact randomly selected, 0.5 (random model) should be within the confidence interval of the predicted accuracy of the 'treated' variable. If that is not the case, the results from the experiment should not be trusted due to randomization issues.\n",
    "* In the following cell blocks, predictions of the 'treated' variable and its corresponding accuracy and confidence interval are computed. A 10-fold cross-validation approach was adopted where the predictions are always on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537a9dea-efb4-4dec-89a0-f8b563c1626c",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "superior-extraction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T01:55:23.955823Z",
     "start_time": "2022-01-28T01:55:23.514020Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'pre_experiment.csv': 0\n",
      "\n",
      "cat_cols ['state', 'type']\n",
      "num_cols ['average_daily_sales_dollar_volume', 'average_daily_sales_transactions', 'age_in_months', 'debt', 'credit_score', 'expense_to_income_ratio', 'offer_size', 'offer_fee']\n",
      "Binarize DataFrameMapper(df_out=True, drop_cols=[],\n",
      "                features=[(['state'], [LabelBinarizer()]),\n",
      "                          (['type'], [LabelBinarizer()]),\n",
      "                          (['average_daily_sales_dollar_volume'], None),\n",
      "                          (['average_daily_sales_transactions'], None),\n",
      "                          (['age_in_months'], None), (['debt'], None),\n",
      "                          (['credit_score'], None),\n",
      "                          (['expense_to_income_ratio'], None),\n",
      "                          (['offer_size'], None), (['offer_fee'], None)],\n",
      "                input_df=True)\n",
      "X        state_AK  state_AL  state_AR  state_AZ  state_CA  state_CO  state_CT  \\\n",
      "0             0         0         0         0         0         0         0   \n",
      "1             0         0         0         0         0         0         0   \n",
      "2             0         0         0         0         0         0         0   \n",
      "3             0         0         0         0         0         0         0   \n",
      "4             0         1         0         0         0         0         0   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "49995         0         0         0         0         0         0         0   \n",
      "49996         0         0         0         0         0         0         0   \n",
      "49997         0         0         0         0         0         0         0   \n",
      "49998         0         0         0         0         0         0         0   \n",
      "49999         0         0         0         0         0         0         0   \n",
      "\n",
      "       state_DC  state_DE  state_FL  ...  type_plumber  type_restaurant  \\\n",
      "0             0         0         0  ...             0                0   \n",
      "1             0         0         0  ...             0                0   \n",
      "2             0         0         0  ...             0                1   \n",
      "3             0         0         0  ...             0                1   \n",
      "4             0         0         0  ...             0                0   \n",
      "...         ...       ...       ...  ...           ...              ...   \n",
      "49995         0         0         0  ...             0                0   \n",
      "49996         0         0         0  ...             0                1   \n",
      "49997         0         0         0  ...             0                1   \n",
      "49998         0         0         0  ...             1                0   \n",
      "49999         0         1         0  ...             0                1   \n",
      "\n",
      "       average_daily_sales_dollar_volume  average_daily_sales_transactions  \\\n",
      "0                                  44.31                              47.0   \n",
      "1                                  10.17                              79.0   \n",
      "2                                1160.25                              14.0   \n",
      "3                                 296.08                              65.0   \n",
      "4                                 238.99                              72.0   \n",
      "...                                  ...                               ...   \n",
      "49995                              36.30                              18.0   \n",
      "49996                             151.59                              17.0   \n",
      "49997                              80.05                              44.0   \n",
      "49998                              64.42                             103.0   \n",
      "49999                              75.05                              81.0   \n",
      "\n",
      "       age_in_months      debt  credit_score  expense_to_income_ratio  \\\n",
      "0               63.5  57957.98         526.0                 0.685382   \n",
      "1               36.6    318.08         709.0                 0.650975   \n",
      "2               72.5  10852.93         633.0                 1.058909   \n",
      "3               83.0  79839.77         706.0                 0.642992   \n",
      "4               57.7  19008.28         608.0                 1.044651   \n",
      "...              ...       ...           ...                      ...   \n",
      "49995           28.2  17723.88         679.0                 0.719576   \n",
      "49996           75.5   1923.41         574.0                 0.753028   \n",
      "49997           29.6    524.55         608.0                 0.606110   \n",
      "49998           45.4    683.49         631.0                 0.381583   \n",
      "49999           35.6    685.35         642.0                 0.691039   \n",
      "\n",
      "       offer_size  offer_fee  \n",
      "0          9134.0      449.0  \n",
      "1          2257.0      143.0  \n",
      "2        277924.0    39118.0  \n",
      "3         73482.0     5024.0  \n",
      "4         48931.0     4963.0  \n",
      "...           ...        ...  \n",
      "49995      5980.0      211.0  \n",
      "49996     39239.0     1899.0  \n",
      "49997     18714.0     1305.0  \n",
      "49998     15054.0      640.0  \n",
      "49999     18537.0      626.0  \n",
      "\n",
      "[50000 rows x 68 columns]\n"
     ]
    }
   ],
   "source": [
    "#!pip install sklearn-pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "dfx = pd.read_csv('pre_experiment.csv')\n",
    "dfy = pd.read_csv('outcomes.csv')\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in 'pre_experiment.csv': {}\\n\".format(dfx.isna().sum().sum()))\n",
    "\n",
    "# Set labels: treated False = 0 / True = 1\n",
    "Y = dfy.treated.astype(int).to_frame() \n",
    "\n",
    "# Set features with proper types\n",
    "xx = dfx.drop(labels=[\"business_id\"], axis=\"columns\")\n",
    "mask = xx.dtypes == 'object'                         # get columns based on data type (state,type)\n",
    "cat_cols = xx.columns[mask].tolist()                 # list with the categorical column labels of the DataFrame\n",
    "print('cat_cols',cat_cols)\n",
    "num_cols = xx.columns[~mask].tolist()                # list with the ordinal columns labels\n",
    "xx[num_cols] = xx[num_cols].astype(float)\n",
    "print('num_cols',num_cols)\n",
    "\n",
    "# Binarize categorical features\n",
    "binarize = DataFrameMapper( \n",
    "    [ ([col], [LabelBinarizer()]) for col in cat_cols ] + \n",
    "    [ ([col], None) for col in num_cols ],\n",
    "    input_df=True,\n",
    "    df_out=True\n",
    ")\n",
    "\n",
    "print('Binarize',binarize)\n",
    "\n",
    "X = binarize.fit_transform(xx)\n",
    "print('X',X)\n",
    "all_cols = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79c4017-5483-4046-bd79-de8a7b8672ca",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "regular-computer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T01:56:52.976940Z",
     "start_time": "2022-01-28T01:55:52.234881Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1, C = 10.0, test score = 0.6526\n",
      "n = 2, C = 100.0, test score = 0.6676\n",
      "n = 3, C = 100.0, test score = 0.6722\n",
      "n = 4, C = 10.0, test score = 0.6634\n",
      "n = 5, C = 10.0, test score = 0.6554\n",
      "n = 6, C = 1.0, test score = 0.669\n",
      "n = 7, C = 1000.0, test score = 0.6632\n",
      "n = 8, C = 1000.0, test score = 0.6664\n",
      "n = 9, C = 1.0, test score = 0.6718\n",
      "n = 10, C = 10.0, test score = 0.6744\n",
      "Probability of predicting treatment based on features = 0.666 +/= 0.007\n"
     ]
    }
   ],
   "source": [
    "# Predict treatment class based on features with Logistic Regression and L2 regularization\n",
    "import pandas as pd\n",
    "import numpy as pf\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Outer 10-fold cross-validation\n",
    "cv_outer = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "accu_outer = []\n",
    "n = 1\n",
    "for id_train, id_test in cv_outer.split(X):\n",
    "    \n",
    "    # set 10 train/test subsets\n",
    "    Xtrain, Xtest = X.iloc[id_train,:], X.iloc[id_test,:]\n",
    "    ytrain, ytest = Y.iloc[id_train,:], Y.iloc[id_test,:]\n",
    "    \n",
    "    # normalize data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(Xtrain)                                                # scaler uses mean and std of training set\n",
    "    xtrain = pd.DataFrame(scaler.transform(Xtrain), columns=all_cols) # results to DataFrame instead of numpy array\n",
    "    xtest  = pd.DataFrame(scaler.transform(Xtest),  columns=all_cols)\n",
    "    \n",
    "    # inner 5-fold cross-validation (fine tune hyperparameter)\n",
    "    cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    # model\n",
    "    model = LogisticRegression(penalty='l2',       # L2 regularization\n",
    "                               random_state=0, \n",
    "                               solver='liblinear', \n",
    "                               max_iter=100)\n",
    "    \n",
    "    # grid search over C (regularization strenght)\n",
    "    grid_C = {'C': list(np.logspace(-1, 4, 6))}\n",
    "    grid_search = GridSearchCV(estimator=model, \n",
    "                               param_grid=grid_C, \n",
    "                               scoring='accuracy', \n",
    "                               cv=cv_inner, \n",
    "                               refit=True,          # Refit an estimator using the best found parameters on the whole dataset (5 fold).\n",
    "                               n_jobs=16)           # Number of jobs to run in parallel \n",
    "    grid_search.fit(xtrain, ytrain.values.ravel())  # apply grid search (ravel -> flatten)\n",
    "    model_best = grid_search.best_estimator_        # save best model\n",
    "    \n",
    "    # predictions\n",
    "    ypred_proba = model_best.predict_proba(xtest)[:,1]  # prob class 1\n",
    "    ypred = model_best.predict(xtest)                   # pred class\n",
    "    accu = accuracy_score(ytest.values.ravel(), ypred)  # true label vs predicted label\n",
    "    \n",
    "    # save relevant data\n",
    "    accu_outer.append(accu)\n",
    "    \n",
    "    # print progress\n",
    "    print(\"n = {}, C = {}, test score = {}\".format(n, grid_search.best_params_['C'], accu))\n",
    "    n += 1\n",
    "    \n",
    "# Print summary\n",
    "print(\"Probability of predicting treatment based on features = {:.3f} +/= {:.3f}\".format(\n",
    "      np.mean(accu_outer), \n",
    "      np.std(accu_outer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6139284-3538-422e-93ad-9915187f8788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T23:40:03.118184Z",
     "start_time": "2022-01-26T23:40:03.115487Z"
    }
   },
   "source": [
    "### <font color=\"blue\">Conclusion: With both L1 and L2 regularization, the models show that the 'treated' variable can be predicted with a 67% accuracy and the random model (0.5) is not within the confidence interval. This means that, based on the features available, the selection of the groups is not completely random and it is biased towards certain features. As a result, a standard proportions test will fail to determine the effect of the treatment on the outcomes. In our particular problem, we can conclude that the results from part a) can not be trusted and we can't tell whether or not the experiment improved the acceptance rate. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61601c5-c31b-4720-a6d1-a7d172092422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "286.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
